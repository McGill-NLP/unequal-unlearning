{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "This is the notebook we use to generate fake biographies for the GPT2 experiments. Adjust the hyperparameters in the first cell and you can then run the entire notebook to have everything uploaded to a huggingface directory of your choice\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "#Set N to the number of biographies to generate (10k is used in the paper)\n",
    "N = 10000\n",
    "\n",
    "#Upsampling factor to upsample the high count split (10 is used in the paper)\n",
    "UPSAMPLING_FACTOR = 10\n",
    "\n",
    "# get HF_TOKEN from environment variable\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "#PATH to save the biographies and qa pairs\n",
    "BIOS_PATH = 'data/biographies.csv'\n",
    "\n",
    "#path to save the train/test splits for the gpt-2 models\n",
    "GPT_SPLITS_PATH = 'data/gpt_splits.csv'\n",
    "\n",
    "#set seed for reproducibility\n",
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to generate data for the biography. I've closely followed the instructions noted in the [[paper]](http://arxiv.org/abs/2309.14316). Speccifically, the details in in section A.1 which I have copied down below. **I found open source github repos for all the required lists, and I'm working off these.**\n",
    "\n",
    "\n",
    "1. **First, middle, and last names** are drawn from pools of 400, 400, and 1000 English names respectively. We apply rejection sampling to ensure all N individuals have unique full names. [[First names Link]](https://gist.githubusercontent.com/JTRNS/6faaf857580eed18aeab6ac9c97993c7/raw/bb5f7ade4f5454ac602c1cf0c00e73f97658243a/first-names.txt), [[Last Names Link]](https://gist.githubusercontent.com/craigh411/19a4479b289ae6c3f6edb95152214efc/raw/d25a1afd3de42f10abdea7740ed098d41de3c330/List%2520of%2520the%25201,000%2520Most%2520Common%2520Last%2520Names%2520(USA)) \n",
    "    \n",
    "\n",
    "2. **Birth years** range from 1900 to 2099, months are selected from the 12 months, and days are chosen between 1 and 28.  \n",
    "3. **Birth cities** are selected from 200 US cities, with their respective state abbreviations, such as Princeton, NJ and Cambridge, MA. [[Cities Link]](https://raw.githubusercontent.com/grammakov/USA-cities-and-states/refs/heads/master/us_cities_states_counties.csv)\n",
    "\n",
    "4. **Universities** are drawn from a list of 300 US institutions. Some may have similar prefixes, like University of California, Berkeley/Irvine/Davis/etc. [[University List Link]](https://gist.githubusercontent.com/dotJoel/90c6acd65331c406d3cb/raw/3f3e5b495b8d6c48f28c43d241075149294f5714/all-colleges.txt)\n",
    "\n",
    "\n",
    "5. **Majors** are selected from 100 common college disciplines, including Computer Science, Physics, and Music. [[Majors Link]](https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/college-majors/majors-list.csv)\n",
    "\n",
    "6. **Employers** are chosen from a list of 263 companies, featuring names like Meta Platforms, Microsoft, and Google. [[Employers Link]](https://raw.githubusercontent.com/EatMoreOranges/Fortune-500-Dataset/refs/heads/main/data/2023-fortune-500-data.csv)\n",
    "\n",
    "7. Pronouns are chosen randomly from [He, She, They]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset numpy seed\n",
    "set_seed(SEED)\n",
    "\n",
    "#Making unique names\n",
    "first_names_url = 'https://gist.githubusercontent.com/JTRNS/6faaf857580eed18aeab6ac9c97993c7/raw/bb5f7ade4f5454ac602c1cf0c00e73f97658243a/first-names.txt'\n",
    "last_names_url = 'https://gist.githubusercontent.com/craigh411/19a4479b289ae6c3f6edb95152214efc/raw/d25a1afd3de42f10abdea7740ed098d41de3c330/List%2520of%2520the%25201,000%2520Most%2520Common%2520Last%2520Names%2520(USA)'\n",
    "cities_url = 'https://raw.githubusercontent.com/grammakov/USA-cities-and-states/refs/heads/master/us_cities_states_counties.csv'\n",
    "\n",
    "response = requests.get(first_names_url)\n",
    "#convert to list of strings\n",
    "first_names = response.content.decode('utf-8').splitlines()\n",
    "\n",
    "middle_names = first_names.copy()\n",
    "\n",
    "response = requests.get(last_names_url)\n",
    "last_names = response.content.decode('utf-8').split(\",\\n\")\n",
    "\n",
    "#make 10000 unique names\n",
    "names = set()\n",
    "while len(names) < N:\n",
    "    first_name = np.random.choice(first_names)\n",
    "    middle_name = np.random.choice(middle_names)\n",
    "    last_name = np.random.choice(last_names)\n",
    "\n",
    "    #make sure no component repeats\n",
    "    if first_name != middle_name and first_name != last_name and middle_name != last_name:\n",
    "        names.add(f\"{first_name} {middle_name} {last_name}\")\n",
    "\n",
    "names = sorted(names)\n",
    "len(np.unique(names)), names[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Birthdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset seed\n",
    "set_seed(SEED)\n",
    "\n",
    "#randomly draw a birth year\n",
    "years = np.random.randint(1900, 2099, size=N)\n",
    "months = np.random.randint(1, 12, size=N)\n",
    "month_verbage = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "months = [month_verbage[i-1] for i in months]\n",
    "days = np.random.randint(1, 28, size=N)\n",
    "\n",
    "birthdays = [f\"{months[i]} {days[i]}, {years[i]}\" for i in range(N)]\n",
    "birthdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Birth Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "set_seed(SEED)\n",
    "\n",
    "cities_url = 'https://raw.githubusercontent.com/grammakov/USA-cities-and-states/refs/heads/master/us_cities_states_counties.csv'\n",
    "\n",
    "cities = pd.read_csv(cities_url,delimiter='|', on_bad_lines='warn')\n",
    "#parse as a csv, skip bad lines['City'].unique()arn')\n",
    "cities = cities [['City','State short']]\n",
    "cities = cities.drop_duplicates()\n",
    "#format into numpy array\n",
    "citynames = (cities['City'].str.title() + ', ' + cities['State short'].str.upper()).to_numpy()\n",
    "\n",
    "#shuffle, subselect 200 cities\n",
    "np.random.shuffle(citynames)\n",
    "citynames = citynames[:200]\n",
    "\n",
    "#now randomly sample N cities\n",
    "citynames = np.random.choice(citynames, size=N)\n",
    "len(np.unique(citynames)), citynames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "set_seed(SEED)\n",
    "\n",
    "universities_url = 'https://gist.githubusercontent.com/dotJoel/90c6acd65331c406d3cb/raw/3f3e5b495b8d6c48f28c43d241075149294f5714/all-colleges.txt'\n",
    "\n",
    "response = requests.get(universities_url)\n",
    "universities = response.content.decode('utf-8').splitlines()\n",
    "#remove content in brackets\n",
    "universities = [re.sub(r'\\(.*?\\)', '', uni).strip() for uni in universities]\n",
    "\n",
    "#shuffle and subsample to 300\n",
    "np.random.shuffle(universities)\n",
    "universities = universities[:300]\n",
    "\n",
    "#now randomly sample N universities\n",
    "universities = np.random.choice(universities, size=N)\n",
    "len(np.unique(universities)), universities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Majors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "major_url = 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/college-majors/majors-list.csv'\n",
    "\n",
    "major_df = pd.read_csv(major_url)\n",
    "#make major title case\n",
    "majors = major_df['Major'].str.title().to_numpy()\n",
    "\n",
    "#suffle and subselect 100\n",
    "np.random.shuffle(majors)\n",
    "majors = majors[:100]\n",
    "\n",
    "#now randomly sample N majors\n",
    "majors = np.random.choice(majors, size=N)\n",
    "len(np.unique(majors)), majors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Employers and Company Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "#url with fortune 500 companies\n",
    "employer_url = 'https://raw.githubusercontent.com/EatMoreOranges/Fortune-500-Dataset/refs/heads/main/data/2023-fortune-500-data.csv'\n",
    "\n",
    "employer_df = pd.read_csv(employer_url)\n",
    "\n",
    "# we  need a company_city attribute. The [city,state_expanded] info is available with the company list, but we need to make it to [city,state_short] for consistency\n",
    "#using the city state data to make a {state_expanded:state_short} dictionary\n",
    "cities_url = 'https://raw.githubusercontent.com/grammakov/USA-cities-and-states/refs/heads/master/us_cities_states_counties.csv'\n",
    "cities = pd.read_csv(cities_url,delimiter='|', on_bad_lines='warn')\n",
    "#parse as a csv, skip bad lines['City'].unique()arn')\n",
    "state_abbrev = cities [['State full','State short']].drop_duplicates()\n",
    "state_abbrev.head()\n",
    "#make this a dictionary\n",
    "state_abbrev = defaultdict(str,zip(state_abbrev['State full'], state_abbrev['State short']))\n",
    "employer_df['State short'] = employer_df['State'].map(state_abbrev)\n",
    "#drop rows with empty state short, this happens when states are expanded differently compared to our dict. I checked and this is about 10 instances\n",
    "employer_df = employer_df[employer_df['State short'] != '']\n",
    "#now making the company city attribute\n",
    "employer_df['company city'] = employer_df.apply(lambda x:f'{x[\"City\"]}, {x[\"State short\"]}', axis=1)\n",
    "employer_df = employer_df[['Company', 'company city']]\n",
    "\n",
    "\n",
    "# #subselect the top 263 as in paper\n",
    "employer_df = employer_df.head(263)\n",
    "#subsample N employers\n",
    "employer_df = employer_df.sample(N,replace=True)\n",
    "\n",
    "\n",
    "companies = employer_df['Company'].to_numpy()\n",
    "company_cities = employer_df['company city'].to_numpy()\n",
    "\n",
    "\n",
    "'''the number of company cities is lower than the number of companies because 24 companies (16%) share HQs in New York City. \n",
    "This is also noted by the paper authors; Their percentage is 13.7%'''\n",
    "\n",
    "len(np.unique(companies)), len(np.unique(company_cities))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# pick personal and possessive pronouns\n",
    "pronouns = {'he':'his', 'she':'her', 'they':'their'}\n",
    "#pick a random pronoun for each name\n",
    "personal_pronoun = np.random.choice(list(pronouns.keys()),size=N)\n",
    "possessive_pronoun = np.array([pronouns[p] for p in personal_pronoun])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now combining all attributes into a dataframe\n",
    "\n",
    "biography_df = pd.DataFrame({'NAME':names, \n",
    "                            'BIRTHDAY':birthdays, \n",
    "                            'LOCATION':citynames,\n",
    "                            'UNIVERSITY':universities,\n",
    "                            'MAJOR':majors,\n",
    "                            'EMPLOYER':companies,\n",
    "                            'EMPLOYER_CITY':company_cities,\n",
    "                            'PERSONAL_PRONOUN':personal_pronoun,\n",
    "                            'POSSESIVE_PRONOUN':possessive_pronoun\n",
    "                            })\n",
    "\n",
    "biography_df.shape, display(biography_df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Templates for bio**S**\n",
    "\n",
    "```\n",
    "Make a prompt that makes Chatgpt give us a bunch of templates. Templates for each attribute are shown. Some postprocessing was done to make the generations fit the format I wanted to process them\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birthday\n",
    "\n",
    "```\n",
    "I want a list of templates  expressing that a person NAME is born on BIRTHDAY. The templates should be distinct. NO other information is provided. Make sure to only depend on X and Y. \n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthday_templates = [\"{NAME} was born on {BIRTHDAY}\",\n",
    "\"{NAME}'s birthdate is {BIRTHDAY}\",\n",
    "\"{NAME} came into the world on {BIRTHDAY}\",\n",
    "\"{NAME} was welcomed into life on {BIRTHDAY}\",\n",
    "\"{NAME}'s journey began on {BIRTHDAY}\",\n",
    "\"On {BIRTHDAY}, {NAME} was born\",\n",
    "\"{BIRTHDAY} marks the birth of {NAME}\",\n",
    "\"{NAME} first saw the light of day on {BIRTHDAY}\",\n",
    "\"{NAME} entered the world on {BIRTHDAY}\",\n",
    "\"{NAME} was given life on {BIRTHDAY}\",\n",
    "\"{BIRTHDAY} is the day {NAME} was born\",\n",
    "\"{NAME} was brought into existence on {BIRTHDAY}\",\n",
    "\"{NAME} was born into the world on {BIRTHDAY}\",\n",
    "\"{NAME} took their first breath on {BIRTHDAY}\",\n",
    "\"The birth of {NAME} took place on {BIRTHDAY}\",\n",
    "\"{NAME} arrived on {BIRTHDAY}\",\n",
    "\"{NAME} was delivered on {BIRTHDAY}\",\n",
    "\"{BIRTHDAY} is when {NAME} was born\",\n",
    "\"{NAME}'s life started on {BIRTHDAY}\",\n",
    "\"{NAME} made their debut in the world on {BIRTHDAY}\",\n",
    "\"{NAME}'s existence began on {BIRTHDAY}\",\n",
    "\"The world first met {NAME} on {BIRTHDAY}\",\n",
    "\"{NAME} made their entrance on {BIRTHDAY}\",\n",
    "\"{BIRTHDAY} marks the moment {NAME} came into the world\",\n",
    "\"{NAME} made their appearance on {BIRTHDAY}\",\n",
    "\"{NAME}'s arrival happened on {BIRTHDAY}\",\n",
    "\"{NAME} was introduced to life on {BIRTHDAY}\",\n",
    "\"{BIRTHDAY} was the day {NAME} entered the world\",\n",
    "\"{NAME}'s first day in the world was {BIRTHDAY}\",\n",
    "\"{NAME} was born to this world on {BIRTHDAY}\",\n",
    "\"The birth of {NAME} occurred on {BIRTHDAY}\",\n",
    "\"{NAME} came to life on {BIRTHDAY}\",\n",
    "\"{BIRTHDAY} is the day that {NAME} was born into this world\",\n",
    "\"{NAME} was born on the day {BIRTHDAY}\",\n",
    "\"{NAME} was brought into life on {BIRTHDAY}\",\n",
    "\"On {BIRTHDAY}, {NAME} was welcomed into existence\",\n",
    "\"{NAME} saw the world for the first time on {BIRTHDAY}\",\n",
    "\"{NAME}'s birth happened on {BIRTHDAY}\",\n",
    "\"{NAME} was born and made their debut on {BIRTHDAY}\",\n",
    "\"{BIRTHDAY} saw the birth of {NAME}\",\n",
    "\"{NAME}'s entrance into life occurred on {BIRTHDAY}\",\n",
    "\"{NAME} took their first steps in life on {BIRTHDAY}\",\n",
    "\"{NAME}'s birth took place on {BIRTHDAY}\",\n",
    "\"{NAME}'s first breath was on {BIRTHDAY}\",\n",
    "\"{NAME} made their entrance into life on {BIRTHDAY}\",\n",
    "\"{NAME} made their arrival on {BIRTHDAY}\",\n",
    "\"{NAME} began their life story on {BIRTHDAY}\",\n",
    "\"The beginning of {NAME}'s life was on {BIRTHDAY}\",\n",
    "\"{NAME}'s journey began on {BIRTHDAY}\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birth Location\n",
    "\n",
    "```\n",
    "I want a list of templates  expressing that a person is born at location LOCATION. The templates should be distinct. If you want to use pronouns, use PERSONAL_PRONOUN and POSSESIVE_PRONOUN as placeholders to be filled in later. Examples:\n",
    "\n",
    "PERSONAL_PRONOUN spent POSSESIVE_PRONOUN early years in LOCATION\n",
    "PERSONAL_PRONOUN celebrates POSSESIVE_PRONOUN birth in LOCATION\n",
    "PERSONAL_PRONOUN owe POSSESIVE_PRONOUN roots to LOCATION\n",
    "PERSONAL_PRONOUN calls LOCATION POSSESIVE_PRONOUN birthplace.\n",
    "\n",
    "Generate 50 such distinct templates in JSON format\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_templates = [\n",
    "    \"{PERSONAL_PRONOUN} was born in {LOCATION}\",\n",
    "    \"{LOCATION} is where {PERSONAL_PRONOUN} came into the world\",\n",
    "    \"{POSSESIVE_PRONOUN} roots lie in {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} entered the world in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} birthplace is {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} hails from {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} has {LOCATION} as {POSSESIVE_PRONOUN} birthplace\",\n",
    "    \"{POSSESIVE_PRONOUN} early years were spent in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} origin is tied to {LOCATION}\",\n",
    "    \"{LOCATION} is where {POSSESIVE_PRONOUN} journey began\",\n",
    "    \"{POSSESIVE_PRONOUN} beginnings trace back to {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} was raised in {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} has strong connections to {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} proudly calls {LOCATION} {POSSESIVE_PRONOUN} hometown\",\n",
    "    \"{LOCATION} is where {PERSONAL_PRONOUN} first saw the light of day\",\n",
    "    \"{PERSONAL_PRONOUN} spent {POSSESIVE_PRONOUN} first days in {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} owes {POSSESIVE_PRONOUN} origins to {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} started life in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} family comes from {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} heritage is rooted in {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} was raised in the heart of {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} story began in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} connection to {LOCATION} runs deep\",\n",
    "    \"{PERSONAL_PRONOUN} was born and raised in {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} was introduced to the world in {LOCATION}\",\n",
    "    \"{LOCATION} holds a special place in {POSSESIVE_PRONOUN} birth story\",\n",
    "    \"{PERSONAL_PRONOUN} took their first breath in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} birth was celebrated in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} arrival into the world happened in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} journey started in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} birthplace is tied to {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} has a deep connection with {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} identity is shaped by {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} early life was shaped by {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} spent {POSSESIVE_PRONOUN} childhood in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} legacy begins in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} heritage stems from {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} carries {LOCATION} within {POSSESIVE_PRONOUN} story\",\n",
    "    \"{POSSESIVE_PRONOUN} life story began in {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} hails from the vibrant streets of {LOCATION}\",\n",
    "    \"{LOCATION} is where {POSSESIVE_PRONOUN} legacy was born\",\n",
    "    \"{PERSONAL_PRONOUN} has deep roots in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} life was first influenced by {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} heart belongs to {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} was first introduced to life in {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} owes {POSSESIVE_PRONOUN} existence to {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} spirit is closely tied to {LOCATION}\",\n",
    "    \"{LOCATION} serves as the birthplace of {PERSONAL_PRONOUN}\",\n",
    "    \"{POSSESIVE_PRONOUN} legacy traces back to {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} is a product of {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} started {POSSESIVE_PRONOUN} story in {LOCATION}\",\n",
    "    \"{POSSESIVE_PRONOUN} journey began in the streets of {LOCATION}\",\n",
    "    \"{PERSONAL_PRONOUN} took their first steps in {LOCATION}\"\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## University\n",
    "\n",
    "```\n",
    "I want a list of templates  expressing that a person went to study at university UNIVERSITY. The templates should be distinct. If you want to use pronouns, use PERSONAL_PRONOUN and POSSESIVE_PRONOUN as placeholders to be filled in later. Examples:\n",
    "\n",
    "PERSONAL_PRONOUN received mentorship and guidance from faculty members at UNIVERSITY\n",
    "PERSONAL_PRONOUN graduated from UNIVERSITY\n",
    "PERSONAL_PRONOUN benefited from the resources and facilities provided by UNIVERSITY\n",
    "PERSONAL_PRONOUN specialized in her field of study at UNIVERSITY\n",
    "\n",
    "Generate 50 such distinct templates in JSON format\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_templates = [\n",
    "  \"{PERSONAL_PRONOUN} pursued higher education at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} attended {UNIVERSITY} to further {POSSESIVE_PRONOUN} academic journey\",\n",
    "  \"{PERSONAL_PRONOUN} enrolled in a degree program at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} was accepted into {UNIVERSITY} for {POSSESIVE_PRONOUN} studies\",\n",
    "  \"{PERSONAL_PRONOUN} honed {POSSESIVE_PRONOUN} skills at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} engaged in rigorous coursework at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} completed {POSSESIVE_PRONOUN} studies at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} joined {UNIVERSITY} to explore {POSSESIVE_PRONOUN} academic interests\",\n",
    "  \"{PERSONAL_PRONOUN} participated in research projects at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} built a strong academic foundation at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} learned from esteemed professors at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} dedicated {POSSESIVE_PRONOUN} time to studies at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} developed expertise in {POSSESIVE_PRONOUN} field at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} pursued {POSSESIVE_PRONOUN} passion for learning at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} engaged in intellectual discourse at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} expanded {POSSESIVE_PRONOUN} knowledge through courses at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} collaborated with peers and professors at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} spent several years studying at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} acquired theoretical and practical knowledge at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} participated in academic conferences while at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} immersed {POSSESIVE_PRONOUN}SELF in campus life at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} took advantage of internship opportunities at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} refined {POSSESIVE_PRONOUN} analytical skills at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} earned a degree from {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} pursued a major in {POSSESIVE_PRONOUN} chosen discipline at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} contributed to student organizations at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} received academic accolades at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} conducted groundbreaking research at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} developed critical thinking skills at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} became well-versed in {POSSESIVE_PRONOUN} subject at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} gained invaluable insights at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} was actively involved in academic discussions at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} benefited from state-of-the-art facilities at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} enhanced {POSSESIVE_PRONOUN} problem-solving abilities at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} explored interdisciplinary studies at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} received mentorship and support from professors at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} dedicated years to mastering {POSSESIVE_PRONOUN} field at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} was a diligent student at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} participated in exchange programs through {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} undertook challenging coursework at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} spent countless hours in the library at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} thrived in an intellectually stimulating environment at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} explored new perspectives through learning at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} refined {POSSESIVE_PRONOUN} research skills at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} broadened {POSSESIVE_PRONOUN} academic horizons at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} was an active member of the academic community at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} built lifelong connections at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} took on leadership roles in student groups at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} pursued {POSSESIVE_PRONOUN} aspirations through education at {UNIVERSITY}\",\n",
    "  \"{PERSONAL_PRONOUN} embarked on {POSSESIVE_PRONOUN} academic journey at {UNIVERSITY}\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major \n",
    "\n",
    "```\n",
    "I want a list of templates  expressing that a person studied major MAJOR at university. The templates should be distinct. If you want to use pronouns, use PERSONAL_PRONOUN and POSSESIVE_PRONOUN as placeholders to be filled in later. Examples:\n",
    "\n",
    "PERSONAL_PRONOUN explored the theoretical aspects of MAJOR\n",
    "PERSONAL_PRONOUN developed a strong foundation in MAJOR\n",
    "PERSONAL_PRONOUN completed a rigorous program in MAJOR\n",
    "PERSONAL_PRONOUN completed POSSESIVE_PRONOUN education with a focus on MAJOR\n",
    "\n",
    "Generate 50 such distinct templates in JSON format\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_templates = [\n",
    "  \"{PERSONAL_PRONOUN} specialized in {MAJOR} at university\",\n",
    "  \"{PERSONAL_PRONOUN} pursued a degree in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} dedicated {POSSESIVE_PRONOUN} studies to {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} engaged in extensive coursework in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} conducted research in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} gained in-depth knowledge of {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} explored both theoretical and practical aspects of {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} mastered core principles of {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} applied critical thinking skills in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} built expertise in {MAJOR} through hands-on experience\",\n",
    "  \"{PERSONAL_PRONOUN} completed advanced studies in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} developed technical skills in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} earned a degree with a concentration in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} was immersed in {MAJOR} during university\",\n",
    "  \"{PERSONAL_PRONOUN} took specialized courses in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} worked on capstone projects related to {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} honed analytical abilities through {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} explored interdisciplinary applications of {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} conducted case studies within {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} refined problem-solving skills in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} collaborated on research projects in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} presented findings on {MAJOR} at academic conferences\",\n",
    "  \"{PERSONAL_PRONOUN} engaged in discussions on contemporary issues in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} completed an internship related to {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} participated in fieldwork for {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} deepened {POSSESIVE_PRONOUN} understanding of {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} examined historical developments in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} studied under renowned professors in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} developed innovative solutions in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} explored emerging trends in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} gained hands-on experience through lab work in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} engaged in data analysis related to {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} pursued a thesis in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} studied the societal impact of {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} applied mathematical concepts to {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} contributed to academic publications in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} examined ethical implications in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} learned industry-standard practices in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} developed programming skills relevant to {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} worked on group projects in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} applied theoretical models in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} participated in case competitions related to {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} refined {POSSESIVE_PRONOUN} communication skills through {MAJOR} coursework\",\n",
    "  \"{PERSONAL_PRONOUN} explored policy implications of {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} examined real-world applications of {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} studied foundational texts in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} engaged in mentorship programs related to {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} learned about cross-disciplinary connections to {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} developed critical perspectives in {MAJOR}\",\n",
    "  \"{PERSONAL_PRONOUN} expanded {POSSESIVE_PRONOUN} academic horizons through {MAJOR}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employer\n",
    "\n",
    "```\n",
    "I want a list of templates  expressing that a person worked for employer EMPLOYER. The templates should be distinct. If you want to use pronouns, use PERSONAL_PRONOUN and POSSESIVE_PRONOUN as placeholders to be filled in later. Examples:\n",
    "\n",
    "PERSONAL_PRONOUN contributed his expertise to EMPLOYER\n",
    "PERSONAL_PRONOUN had a job at EMPLOYER\n",
    "PERSONAL_PRONOUN had employment prospects at EMPLOYER\n",
    "PERSONAL_PRONOUN had a professional role at EMPLOYER\n",
    "\n",
    "Generate 50 such distinct templates in JSON format\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer_templates = [\n",
    "  \"{PERSONAL_PRONOUN} was employed at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} built {POSSESIVE_PRONOUN} career at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} gained valuable experience at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} worked as a professional at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} served in a key role at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} took on responsibilities at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} played a vital role at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} contributed to projects at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} was part of the team at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} engaged in professional activities at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} developed skills while working at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} spent years working at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} advanced {POSSESIVE_PRONOUN} career at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} was a dedicated employee at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} took part in major initiatives at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} was an integral part of {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} held a position at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} pursued {POSSESIVE_PRONOUN} profession at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} worked on high-impact projects at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} gained industry knowledge at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} developed expertise through {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} honed {POSSESIVE_PRONOUN} skills at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} was a valued team member at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} made significant contributions to {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} played a key part in operations at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} achieved professional growth at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} collaborated with colleagues at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} worked diligently at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} held an influential role at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} delivered results at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} was involved in strategic planning at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} managed projects at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} oversaw critical tasks at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} thrived in {POSSESIVE_PRONOUN} career at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} worked to achieve success at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} established {POSSESIVE_PRONOUN} professional reputation at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} contributed to the mission of {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} gained hands-on experience at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} executed major assignments at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} was recognized for {POSSESIVE_PRONOUN} contributions at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} had a rewarding career at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} took on leadership responsibilities at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} was a key contributor at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} delivered excellence at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} brought innovation to {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} supported business goals at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} was committed to {POSSESIVE_PRONOUN} work at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} provided expertise at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} was known for {POSSESIVE_PRONOUN} dedication at {EMPLOYER}\",\n",
    "  \"{PERSONAL_PRONOUN} played a crucial role in success at {EMPLOYER}\"\n",
    "]\n",
    "## Employer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employer City\n",
    "\n",
    "```\n",
    "I want a list of templates  expressing that a person worked in CITY. The templates should be distinct. If you want to use pronouns, use PERSONAL_PRONOUN and POSSESIVE_PRONOUN as placeholders to be filled in later. Examples:\n",
    "\n",
    "PERSONAL_PRONOUN was employed in CITY\n",
    "PERSONAL_PRONOUN acquired industry knowledge while working in CITY\n",
    "POSSESIVE_PRONOUN work was based in CITY\n",
    "POSSESIVE_PRONOUN projects were located in CITY\n",
    "PERSONAL_PRONOUN gained work experience in CITY\n",
    "\n",
    "Generate 50 such distinct templates in JSON format\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer_city_templates = [\n",
    "  \"{PERSONAL_PRONOUN} worked professionally in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} established {POSSESIVE_PRONOUN} career in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} took on professional responsibilities in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} was engaged in work assignments in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} contributed to industry growth in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} advanced {POSSESIVE_PRONOUN} professional journey in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} built {POSSESIVE_PRONOUN} expertise in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} participated in business operations in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} developed professional skills in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} expanded {POSSESIVE_PRONOUN} network while working in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} managed key projects in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} collaborated with industry leaders in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} was actively involved in business activities in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} contributed to innovative solutions in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} provided expertise in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} took on leadership roles in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} played a crucial role in business success in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} handled professional responsibilities in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} navigated the corporate landscape in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} worked with diverse teams in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} pursued professional growth opportunities in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} engaged in entrepreneurial ventures in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} took part in major business initiatives in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} achieved career milestones in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} delivered results for organizations in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} led strategic efforts in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} participated in groundbreaking projects in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} gained valuable insights through work in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} contributed to economic development in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} enhanced {POSSESIVE_PRONOUN} skill set in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} thrived in the work environment of {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} took part in cross-functional teams in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} strengthened {POSSESIVE_PRONOUN} professional profile in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} played a key role in the workforce of {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} built professional relationships in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} collaborated on high-profile projects in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} gained industry recognition through work in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} handled business operations in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} engaged in consulting work in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} pursued career opportunities in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} achieved professional success in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} worked across various sectors in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} developed innovative strategies in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} contributed to corporate growth in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} was part of a thriving work culture in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} applied {POSSESIVE_PRONOUN} expertise to projects in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} supported key business initiatives in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} maintained a strong work presence in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} excelled in {POSSESIVE_PRONOUN} profession in {EMPLOYER_CITY}\",\n",
    "  \"{PERSONAL_PRONOUN} established a successful work history in {EMPLOYER_CITY}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Biographies\n",
    "Take a random template from each of the templates and fill in the placeholders with the values from the dataframe. **The biography contains six sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "biographies = []\n",
    "for row in biography_df.itertuples():\n",
    "    # Pick a random template from each of the templates\n",
    "    biography_templates = [np.random.choice(birthday_templates), \n",
    "                          np.random.choice(city_templates), \n",
    "                          np.random.choice(university_templates), \n",
    "                          np.random.choice(major_templates), \n",
    "                          np.random.choice(employer_templates), \n",
    "                          np.random.choice(employer_city_templates)]\n",
    "    sentences = []\n",
    "    for template in biography_templates:\n",
    "        # Fill in the placeholders with the values from the dataframe, make sure to capitalize the first letter of each sentence\n",
    "        complete_sentence = template.format(**row._asdict())\n",
    "        complete_sentence = complete_sentence[0].capitalize()+complete_sentence[1:]\n",
    "        sentences.append(complete_sentence)\n",
    "\n",
    "    biography = \". \".join(sentences) \n",
    "    if row.PERSONAL_PRONOUN == 'they':\n",
    "        biography = biography.replace('they was', 'they were').replace('They was', 'They were')\n",
    "        biography = biography.replace('they is', 'they were').replace('They is', 'They were')\n",
    "    biographies.append(biography)\n",
    "\n",
    "\n",
    "biography_df['BIOGRAPHY'] = biographies\n",
    "\n",
    "#shuffle dataframe\n",
    "biography_df = biography_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "display(biography_df.head())\n",
    "\n",
    "#upload to HF\n",
    "dataset = Dataset.from_pandas(biography_df)\n",
    "dataset.push_to_hub(BIOS_PATH,private=False, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making splits, upsampling high-count and making QA instances\n",
    "\n",
    "For Unlearning, we need to make a dataset with four splits: forget_high_count, forget_low_count, retain, and utility. \n",
    "the high count should be scaled 10 times. utility 5 times. retain 3 times . My idea here is to not copy the biographies for upsampling anymore. This promotes verbatim memorization. We should reconstruct a new biography for the high_count dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that remakes biographies for upsampling\n",
    "def make_biography(row):\n",
    "    biography_templates = [np.random.choice(birthday_templates), \n",
    "                        np.random.choice(city_templates), \n",
    "                        np.random.choice(university_templates), \n",
    "                        np.random.choice(major_templates), \n",
    "                        np.random.choice(employer_templates), \n",
    "                        np.random.choice(employer_city_templates)]\n",
    "    sentences = []\n",
    "    for template in biography_templates:\n",
    "        # Fill in the placeholders with the values from the dataframe, make sure to capitalize the first letter of each sentence\n",
    "        complete_sentence = template.format(**dict(row))\n",
    "        complete_sentence = complete_sentence[0].capitalize()+complete_sentence[1:]\n",
    "        sentences.append(complete_sentence)\n",
    "\n",
    "    biography = \". \".join(sentences) \n",
    "    if row['PERSONAL_PRONOUN'] == 'they':\n",
    "        # biography = biography.replace('they was', 'they were')\n",
    "        biography = biography.replace('they was', 'they were').replace('They was', 'They were')\n",
    "        biography = biography.replace('they is', 'they were').replace('They is', 'They were')\n",
    "    return {'BIOGRAPHY' : biography}\n",
    "\n",
    "\n",
    "\n",
    "#Templates for making questions\n",
    "question_templates = [\n",
    "    \"What is the birth date of {NAME}? {BIRTHDAY}.\",\n",
    "    \"What is the birth city of {NAME}? {LOCATION}.\",\n",
    "    \"Which university did {NAME} study? {UNIVERSITY}.\",\n",
    "    \"What major did {NAME} study? {MAJOR}.\",\n",
    "    \"Which company did {NAME} work for? {EMPLOYER}.\",\n",
    "    \"Where did {NAME} work? {EMPLOYER_CITY}.\"]\n",
    "\n",
    "\n",
    "#make a qa dataset with columns name, question, answer\n",
    "def make_qa_dataset(row):\n",
    "    qa_pairs = []\n",
    "    for question_template in question_templates:\n",
    "        qa = question_template.format(**dict(row))\n",
    "        question = qa.split('?')[0]\n",
    "        answer = qa.split('?')[1].strip('.')\n",
    "        qa_pairs.append({'NAME': row['NAME'], 'question': question, 'answer': answer, 'qa': qa})\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making splits, upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(BIOS_PATH, split='train')\n",
    "\n",
    "\n",
    "# First split the dataset into four equal parts : retain, forget_high_count, forget_low_count, utility\n",
    "dataset_dict = {}\n",
    "dataset_dict['retain_biography'], half_split_1 = dataset.train_test_split(test_size=0.5, shuffle=False).values()\n",
    "dataset_dict['forget_high_count_biography'], half_split_2 = half_split_1.train_test_split(test_size=0.6666, shuffle= False).values()\n",
    "dataset_dict['forget_low_count_biography'], dataset_dict['utility_biography'] = half_split_2.train_test_split(test_size=0.5, shuffle= False).values()\n",
    "\n",
    "#make sure there is no name overlap between any sets    \n",
    "for split in dataset_dict.values():\n",
    "    for other_split in dataset_dict.values():\n",
    "        if split != other_split:\n",
    "            assert not set(split['NAME']).intersection(set(other_split['NAME']))\n",
    "\n",
    "qa_dataset_dict = {}\n",
    "for split_name,split in dataset_dict.items():\n",
    "    qa_pairs = []\n",
    "    for row in tqdm(split):\n",
    "        qa_pairs.extend(make_qa_dataset(row))\n",
    "    qa_dataset_dict[split_name.replace('_biography','_qa')] = Dataset.from_list(qa_pairs)\n",
    "\n",
    "dataset_dict.update(qa_dataset_dict)\n",
    "\n",
    "\n",
    "#Forget_high count should be scaled 10 times.\n",
    "#defauldict that returns 1 if the key is not in the dict\n",
    "repeat_dict = defaultdict(lambda: 1)\n",
    "repeat_dict['forget_high_count_biography'] = UPSAMPLING_FACTOR\n",
    "\n",
    "#Upsampling Biographies for the high count set\n",
    "unlearn_dataset_dict = {}\n",
    "for split_name, split in dataset_dict.items():\n",
    "    if repeat_dict[split_name] > 1:\n",
    "        unlearn_dataset_dict[split_name] = concatenate_datasets( [split.map(make_biography, remove_columns='BIOGRAPHY', load_from_cache_file=False) for i in range(repeat_dict[split_name])])\n",
    "    else:\n",
    "        unlearn_dataset_dict[split_name] = split\n",
    "\n",
    "unlearn_dataset_dict =  DatasetDict(unlearn_dataset_dict)\n",
    "\n",
    "\n",
    "''' Code to make sure that the BIO/QA sets on same splits use the same names, and that different splits have no overlap'''\n",
    "# for split_name, split in unlearn_dataset_dict.items():\n",
    "#     for other_split_name, other_split in unlearn_dataset_dict.items():\n",
    "#         #we don't know if it is a qa or a biography set, so we take rootgroup to be the first part of the name\n",
    "#         split_group = '_'.join(split_name.split('_')[:-1])\n",
    "#         other_split_group = '_'.join(other_split_name.split('_')[:-1])\n",
    "#         if split_group == other_split_group:\n",
    "#             #if the groups are the same, the names should be the same\n",
    "#             assert set(split['NAME']) == set(other_split['NAME'])\n",
    "#         if split_group != other_split_group:\n",
    "#             #if the groups are different, the names should have no overlap\n",
    "#             assert not set(split['NAME']).intersection(set(other_split['NAME']))\n",
    "\n",
    "\n",
    "\n",
    "#now combine all the biography splits to make the training biography set\n",
    "unlearn_dataset_dict['fake_biographies_train'] = concatenate_datasets([split for split_name,split in unlearn_dataset_dict.items() if 'biography' in split_name])\n",
    "\n",
    "#QA set for training is the retain set\n",
    "unlearn_dataset_dict['fake_biographies_qa_train'] = unlearn_dataset_dict['retain_qa']\n",
    "\n",
    "#make EXTRA sure that the utility, highcount and low count questions are not in the training set\n",
    "assert not set(unlearn_dataset_dict['fake_biographies_qa_train']['question']).intersection(set(unlearn_dataset_dict['utility_qa']['question']))\n",
    "assert not set(unlearn_dataset_dict['fake_biographies_qa_train']['question']).intersection(set(unlearn_dataset_dict['forget_high_count_qa']['question']))\n",
    "assert not set(unlearn_dataset_dict['fake_biographies_qa_train']['question']).intersection(set(unlearn_dataset_dict['forget_low_count_qa']['question']))\n",
    "\n",
    "#push the dataset to the hub\n",
    "for data_name,data in unlearn_dataset_dict.items():\n",
    "    print(data_name)\n",
    "    data.push_to_hub(GPT_SPLITS_PATH,\n",
    "                    private=True,\n",
    "                    config_name=data_name,\n",
    "                    split='train',\n",
    "                    token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Forget sets (split by attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The idea is to split the questions in the high,low and retain counts by attribute. In the paper, we unlearn one attribute of the high/low count split.\n",
    "Retain samples are also sourced from the same attribute during unlearning.\n",
    "'''\n",
    "\n",
    "for split in ['forget_high_count_qa', 'forget_low_count_qa', 'retain_qa']:\n",
    "    dataset = load_dataset(GPT_SPLITS_PATH, split)[\"train\"]\n",
    "    #now take apart the atrributes seperately. The questions are in order of birthday, location, university, employer, employer_loc we can just take every 6th question from a start loc\n",
    "    question_location = {'BIRTHDAY':0, 'LOCATION': 1, 'UNIVERSITY': 2, 'MAJOR':3,  'EMPLOYER': 4, 'EMPLOYER_LOC': 5}\n",
    "    for question_type, location in question_location.items():\n",
    "        dataset_subset = dataset.select(range(location,len(dataset),6))\n",
    "        # Drop duplicate rows\n",
    "        unique_rows = set()\n",
    "        dataset_subset = dataset_subset.map(lambda x: {'answer': x['answer'].strip() + '.'}, remove_columns = 'answer')\n",
    "        dataset_subset = dataset_subset.filter(lambda row: tuple(row.values()) not in unique_rows and not unique_rows.add(tuple(row.values())))\n",
    "        dataset_subset.push_to_hub(GPT_SPLITS_PATH, f\"{split}_{question_type.lower()}\", split = 'train', token=HF_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tofu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
