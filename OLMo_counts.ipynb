{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```This Notebook shows how we compiled the ngram counts for the QA datasets from DOLMA```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country-Capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/241 [00:33<2:13:37, 33.41s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Counting frequencies of countries and capitals on Dolma\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "#load countries and capitals from github\n",
    "country_capitals = pd.read_csv(\"https://raw.githubusercontent.com/icyrockcom/country-capitals/refs/heads/master/data/country-list.csv\")[['country','capital']]\n",
    "\n",
    "#Remove instances where country and capital are the same if it exists\n",
    "country_capitals = country_capitals[country_capitals['country'] != country_capitals['capital']]\n",
    "\n",
    "#now go through each row and count the frequency of the country and capital with infinigram\n",
    "for row in tqdm(country_capitals.itertuples(), total=len(country_capitals)):\n",
    "    country = row.country\n",
    "    capital = row.capital\n",
    "    count = 0\n",
    "    try:\n",
    "        #Run frequency count with cased variations of country and capital and their combinations:\n",
    "        for case in [f'{country.title()} AND {capital.title()}', f'{country.lower()} AND {capital.lower()}', f'{country} AND {capital.lower()}', f'{country.lower()} AND {capital}']:   \n",
    "            payload = {\n",
    "                'index': 'v4_dolma-v1_7_llama',\n",
    "                'query_type': 'count',\n",
    "                'query': f'{case}'}\n",
    "            result = requests.post('https://api.infini-gram.io/', json=payload).json()\n",
    "            count += result['count']\n",
    "        country_capitals.loc[row.Index, 'count'] = count\n",
    "    #except keyboard interrupt\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "    #print the error\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {country} and {capital}: {e}\")\n",
    "        continue\n",
    "\n",
    "#sort by count and reset index\n",
    "country_capitals.sort_values(by='count', ascending=False, inplace=True)\n",
    "country_capitals.reset_index(drop=True, inplace=True)\n",
    "\n",
    "country_capitals.to_csv(\"country-capitals-freq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making QA data, paraphrases with Chatgpt (Provide OPENAI API key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = YOUR-OPENAI-API-KEY\n",
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "# Read the country capitals data\n",
    "country_capitals = pd.read_csv(\"country-capitals-freq.csv\")\n",
    "\n",
    "# Shuffle the dataframe\n",
    "country_capitals = country_capitals.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "dataset = []\n",
    "# Load existing data if any\n",
    "if os.path.exists(\"capital_questions_dataset.json\"):\n",
    "    with open(\"capital_questions_dataset.json\", \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "processed_countries = {item[\"country\"] for item in dataset}\n",
    "\n",
    "for row in tqdm(country_capitals.itertuples(), total=len(country_capitals)):\n",
    "    country = row.country\n",
    "    capital = row.capital\n",
    "    count = row.count\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if country in processed_countries:\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing {country} with capital {capital} (frequency count: {count})\")\n",
    "    \n",
    "    prompt = f\"\"\"For the country {country} with capital {capital}, generate:\n",
    "    1. A direct question asking what the capital is and its answer\n",
    "    2. A paraphrased version of the capital question that tests the same knowledge\n",
    "    \n",
    "    Format the output as a JSON with the following structure:\n",
    "    {{\n",
    "        \"direct_question\": \"question\",\n",
    "        \"direct_answer\": \"answer\",\n",
    "        \"paraphrased_question\": \"question\",\n",
    "        \"paraphrased_answer\": \"answer\"\n",
    "    }}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"gpt-4\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        result[\"country\"] = country\n",
    "        result[\"capital\"] = capital\n",
    "        dataset.append(result)\n",
    "        \n",
    "        # Save after each successful addition\n",
    "        with open(\"capital_questions_dataset.json\", \"w\") as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {country}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Generated dataset with {len(dataset)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making count buckets, pushing to HF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datasets\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "HF_DATASET_NAME = provide HF dataset name\n",
    "\n",
    "# Load and process your custom dataset\n",
    "with open(\"capital_questions_dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Calculate count thresholds for three equal-sized buckets\n",
    "count_thresholds = df['count'].quantile([0.33, 0.67])\n",
    "\n",
    "# Create three buckets based on count\n",
    "low_count = df[df['count'] <= count_thresholds[0.33]]\n",
    "medium_count = df[(df['count'] > count_thresholds[0.33]) & (df['count'] <= count_thresholds[0.67])]\n",
    "high_count = df[df['count'] > count_thresholds[0.67]]\n",
    "\n",
    "# Convert each bucket to HuggingFace datasets and push to hub\n",
    "low_count_dataset = Dataset.from_pandas(low_count)\n",
    "medium_count_dataset = Dataset.from_pandas(medium_count)\n",
    "high_count_dataset = Dataset.from_pandas(high_count)\n",
    "\n",
    "print(f\"Low count bucket (count <= {count_thresholds[0.33]:.1f}): {len(low_count)} samples\")\n",
    "print(f\"Medium count bucket ({count_thresholds[0.33]:.1f} < count <= {count_thresholds[0.67]:.1f}): {len(medium_count)} samples\") \n",
    "print(f\"High count bucket (count > {count_thresholds[0.67]:.1f}): {len(high_count)} samples\")\n",
    "\n",
    "def create_count_specific_datasets(data, count_range):\n",
    "    # Create DataFrames for each subset\n",
    "    direct_df = pd.DataFrame({\n",
    "        'question': [d['question'] for d in data],\n",
    "        'answer': [d['answer'] for d in data],\n",
    "        'country': [d['country'] for d in data],\n",
    "        'capital': [d['capital'] for d in data], \n",
    "        'count': [d['count'] for d in data]\n",
    "    })\n",
    "\n",
    "    paraphrase_df = pd.DataFrame({\n",
    "        'question': [d['paraphrased_question'] for d in data],\n",
    "        'answer': [d['answer'] for d in data],\n",
    "        'country': [d['country'] for d in data],\n",
    "        'capital': [d['capital'] for d in data],\n",
    "        'count': [d['count'] for d in data]\n",
    "    })\n",
    "\n",
    "    # Convert DataFrames to Datasets\n",
    "    direct_dataset = Dataset.from_pandas(direct_df)\n",
    "    paraphrase_dataset = Dataset.from_pandas(paraphrase_df)\n",
    "    \n",
    "    return direct_dataset, paraphrase_dataset\n",
    "\n",
    "# Create datasets for each count bucket\n",
    "low_direct, low_paraphrase = create_count_specific_datasets(low_count.to_dict('records'), 'low')\n",
    "med_direct, med_paraphrase = create_count_specific_datasets(medium_count.to_dict('records'), 'medium')\n",
    "high_direct, high_paraphrase  = create_count_specific_datasets(high_count.to_dict('records'), 'high')\n",
    "\n",
    "# Create datasets for all data (I.E HIGH + MEDIUM + LOW)\n",
    "all_direct, all_paraphrase = create_count_specific_datasets(df.to_dict('records'), 'all')\n",
    "\n",
    "# Load auxiliary datasets\n",
    "world_facts = datasets.load_dataset('locuslab/TOFU', \"world_facts\")[\"train\"]\n",
    "world_facts = world_facts.remove_columns(['option1', 'option2', 'option3', 'option4'])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'forget_low_count': low_direct,\n",
    "    'forget_low_count_paraphrased': low_paraphrase,\n",
    "    'forget_medium_count': med_direct,\n",
    "    'forget_medium_count_paraphrased': med_paraphrase,\n",
    "    'forget_high_count': high_direct,\n",
    "    'forget_high_count_paraphrased': high_paraphrase,\n",
    "    'forget_all': all_direct,\n",
    "    'forget_all_paraphrased': all_paraphrase, \n",
    "    'real_authors': real_authors,\n",
    "    'world_facts': world_facts\n",
    "})\n",
    "\n",
    "for config_name, dataset_ in dataset_dict.items():\n",
    "    dataset_.push_to_hub(HF_DATASET_NAME, config_name=config_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
